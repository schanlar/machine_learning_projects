{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05421850",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "A large technology firm have been hacked! Luckily their forensic engineers have grabbed valuable data about the hacks, including information like session time,locations, wpm typing speed, etc. The forensic engineer relates to you what she has been able to figure out so far, she has been able to grab meta data of each session that the hackers used to connect to their servers. These are the features of the data:\n",
    "\n",
    "* 'Session_Connection_Time': How long the session lasted in minutes\n",
    "\n",
    "\n",
    "* 'Bytes Transferred': Number of MB transferred during session\n",
    "\n",
    "\n",
    "* 'Kali_Trace_Used': Indicates if the hacker was using Kali Linux\n",
    "\n",
    "\n",
    "* 'Servers_Corrupted': Number of server corrupted during the attack\n",
    "\n",
    "\n",
    "* 'Pages_Corrupted': Number of pages illegally accessed\n",
    "\n",
    "\n",
    "* 'Location': Location attack came from (Probably useless because the hackers used VPNs)\n",
    "\n",
    "\n",
    "* 'WPM_Typing_Speed': Their estimated typing speed based on session logs.\n",
    "\n",
    "The technology firm has 3 potential hackers that perpetrated the attack. Their certain of the first two hackers but they aren't very sure if the third hacker was involved or not.\n",
    "\n",
    "One last key fact, the forensic engineer knows that the hackers trade off attacks. Meaning they should each have roughly the same amount of attacks. For example if there were 100 total attacks, then in a 2 hacker situation each should have about 50 hacks, in a three hacker situation each would have about 33 hacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff3adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/21 20:37:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('hack').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea570234",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"hack_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43578fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f40908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22508afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/21 20:37:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
      "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       NULL|57.342395209580864|\n",
      "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       NULL| 13.41106336843464|\n",
      "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
      "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb17974",
   "metadata": {},
   "source": [
    "## Data transformation and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3badd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schanlar/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bf7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Session_Connection_Time\",\n",
    "        \"Bytes Transferred\",\n",
    "        \"Kali_Trace_Used\",\n",
    "        \"Servers_Corrupted\",\n",
    "        \"Pages_Corrupted\",\n",
    "        \"WPM_Typing_Speed\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd77423",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d54502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c96eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67a37b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339affbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e2bf9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scaler.fit(final_data).transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e9f3ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[8.0,391.09,1.0,2...|[0.56785108466505...|\n",
      "|[20.0,720.99,0.0,...|[1.41962771166263...|\n",
      "|[31.0,356.32,1.0,...|[2.20042295307707...|\n",
      "|[2.0,228.08,1.0,2...|[0.14196277116626...|\n",
      "|[20.0,408.5,0.0,3...|[1.41962771166263...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9d1db",
   "metadata": {},
   "source": [
    "## Create a Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a00dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31475140",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2 = KMeans(featuresCol='scaledFeatures', k=2)\n",
    "kmeans3 = KMeans(featuresCol='scaledFeatures', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103f78b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/21 20:37:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/01/21 20:37:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "k2_model = kmeans2.fit(scaled_data)\n",
    "k3_model = kmeans3.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b818e03",
   "metadata": {},
   "source": [
    "## Evaluating K2 v K3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4284545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45ee027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost for KMeans with 2 clusters: 601.77\n",
      "Training cost for KMeans with 3 clusters: 434.76\n"
     ]
    }
   ],
   "source": [
    "# wssse\n",
    "print(f\"Training cost for KMeans with 2 clusters: {k2_model.summary.trainingCost:.2f}\")\n",
    "print(f\"Training cost for KMeans with 3 clusters: {k3_model.summary.trainingCost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a915c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k2_model_preds = k2_model.transform(scaled_data)\n",
    "k3_model_preds = k3_model.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3bbe758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|            features|      scaledFeatures|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|[8.0,391.09,1.0,2...|[0.56785108466505...|         0|\n",
      "|[20.0,720.99,0.0,...|[1.41962771166263...|         0|\n",
      "|[31.0,356.32,1.0,...|[2.20042295307707...|         0|\n",
      "|[2.0,228.08,1.0,2...|[0.14196277116626...|         0|\n",
      "|[20.0,408.5,0.0,3...|[1.41962771166263...|         0|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k2_model_preds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15616482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|            features|      scaledFeatures|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|[8.0,391.09,1.0,2...|[0.56785108466505...|         1|\n",
      "|[20.0,720.99,0.0,...|[1.41962771166263...|         1|\n",
      "|[31.0,356.32,1.0,...|[2.20042295307707...|         1|\n",
      "|[2.0,228.08,1.0,2...|[0.14196277116626...|         1|\n",
      "|[20.0,408.5,0.0,3...|[1.41962771166263...|         1|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k3_model_preds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ce567ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86277c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K2 model silhouette with squared euclidean distance : 0.668\n",
      "K3 model silhouette with squared euclidean distance : 0.304\n"
     ]
    }
   ],
   "source": [
    "k2_model_silhouette = evaluator.evaluate(k2_model_preds)\n",
    "k3_model_silhouette = evaluator.evaluate(k3_model_preds)\n",
    "\n",
    "print(f\"K2 model silhouette with squared euclidean distance : {k2_model_silhouette:.3f}\")\n",
    "print(f\"K3 model silhouette with squared euclidean distance : {k3_model_silhouette:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b611b6",
   "metadata": {},
   "source": [
    "Not much to be gained from the WSSSE, after all, we would expect that as K increases, the WSSSE decreases.\n",
    "\n",
    "However, we know that the attacks should be evenly numbered between the hackers! Let's check with the transform and prediction columns that result form this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86207538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k2_model_preds.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a8ebd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         2|   79|\n",
      "|         0|   88|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k3_model_preds.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5a078",
   "metadata": {},
   "source": [
    "**Conclusion**: It was 2 hackers, in fact, our clustering algorithm created two equally sized clusters with K=2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60ffff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/22 22:32:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('tools').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e657d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schanlar/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7845b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame full of sentences\n",
    "sen_df = spark.createDataFrame(\n",
    "    [\n",
    "    (0, \"PySpark is a powerful tool for big data processing\"),\n",
    "    (1, \"Creating DataFrames is easy with PySpark\"),\n",
    "    (2, \"SparkSession provides a unified entry point for reading data and executing queries\"),\n",
    "    (3, \"Logistic,regression,models,are,neat\"),\n",
    "    (4, \"Finally, this is the last sentence\")\n",
    "    ], \n",
    "    ['id', 'sentence']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "078a9df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|PySpark is a powe...|\n",
      "|  1|Creating DataFram...|\n",
      "|  2|SparkSession prov...|\n",
      "|  3|Logistic,regressi...|\n",
      "|  4|Finally, this is ...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b69213",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ce621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de2c8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf : user defined function\n",
    "count_tokens = udf(lambda words:len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1543ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(sen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff34a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|PySpark is a powe...|[pyspark, is, a, ...|\n",
      "|  1|Creating DataFram...|[creating, datafr...|\n",
      "|  2|SparkSession prov...|[sparksession, pr...|\n",
      "|  3|Logistic,regressi...|[logistic,regress...|\n",
      "|  4|Finally, this is ...|[finally,, this, ...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The way it is displayed makes it unclear\n",
    "# if the 4th row has been succesfully tokenized, or if it is a single long string\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "132be784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|PySpark is a powe...|[pyspark, is, a, ...|     9|\n",
      "|  1|Creating DataFram...|[creating, datafr...|     6|\n",
      "|  2|SparkSession prov...|[sparksession, pr...|    12|\n",
      "|  3|Logistic,regressi...|[logistic,regress...|     1|\n",
      "|  4|Finally, this is ...|[finally,, this, ...|     6|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 4th row could not be split because there was no white space to split on\n",
    "# The 5th row was splitted on white space resulting in the first element being \"finally,\"\n",
    "tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a665a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split not only on white space, but also on commas\n",
    "regex_tokenized = regex_tokenizer.transform(sen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63021499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|PySpark is a powe...|[pyspark, is, a, ...|     9|\n",
      "|  1|Creating DataFram...|[creating, datafr...|     6|\n",
      "|  2|SparkSession prov...|[sparksession, pr...|    12|\n",
      "|  3|Logistic,regressi...|[logistic, regres...|     5|\n",
      "|  4|Finally, this is ...|[finally, this, i...|     6|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f4b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b3c345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame(\n",
    "    [\n",
    "        (0, \"I saw the green horse\"),\n",
    "        (1, \"Mary had a little lamb\")\n",
    "    ],\n",
    "    [\"id\", \"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a979387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|I saw the green h...|\n",
      "|  1|Mary had a little...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d22bb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame_tokenized = tokenizer.transform(sentenceDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eeb467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|I saw the green h...|[i, saw, the, gre...|\n",
      "|  1|Mary had a little...|[mary, had, a, li...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "922c7083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/22 22:32:49 WARN StopWordsRemover: Default locale set was [en_GR]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca662996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|               words|            filtered|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|I saw the green h...|[i, saw, the, gre...| [saw, green, horse]|\n",
      "|  1|Mary had a little...|[mary, had, a, li...|[mary, little, lamb]|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(sentenceDataFrame_tokenized).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
